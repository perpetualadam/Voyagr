# Phase 5: Database Query Optimization

**Status**: IN PROGRESS  
**Date**: 2025-11-15  
**Target**: Reduce query time by 50%  

---

## ðŸŽ¯ DATABASE OPTIMIZATION STRATEGIES

### 1. Query Analysis (QUICK WIN)
**Issue**: Slow queries not identified
**Solution**: Enable query logging and analysis
**Expected Impact**: Identify bottlenecks
**Effort**: 15 minutes

```python
import time
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def execute_query(query, params=()):
    """Execute query with timing."""
    start = time.time()
    cursor = conn.cursor()
    cursor.execute(query, params)
    elapsed = time.time() - start
    
    if elapsed > 0.1:  # Log slow queries
        logger.warning(f"Slow query ({elapsed:.2f}s): {query}")
    
    return cursor.fetchall()
```

**Benefits**:
- Identify slow queries
- Understand bottlenecks
- Prioritize optimization

### 2. Index Optimization (QUICK WIN)
**Issue**: Missing indexes on frequently queried fields
**Solution**: Add indexes for common queries
**Expected Impact**: 50-70% query time reduction
**Effort**: 30 minutes

```sql
-- Add indexes for frequently queried fields
CREATE INDEX idx_trip_user_date ON trip_history(user_id, date);
CREATE INDEX idx_trip_start_end ON trip_history(start_lat, start_lon, end_lat, end_lon);
CREATE INDEX idx_hazard_location ON hazards(latitude, longitude);
CREATE INDEX idx_hazard_type ON hazards(hazard_type);
CREATE INDEX idx_route_cache_start_end ON route_cache(start_lat, start_lon, end_lat, end_lon);
CREATE INDEX idx_vehicle_user ON vehicles(user_id);
CREATE INDEX idx_charging_location ON charging_stations(latitude, longitude);
```

**Benefits**:
- 50-70% faster queries
- Better scalability
- Reduced CPU usage

### 3. Query Optimization (MEDIUM)
**Issue**: Inefficient query patterns
**Solution**: Optimize query structure
**Expected Impact**: 30-50% query time reduction
**Effort**: 1 hour

```python
# Before: N+1 query problem
trips = db.execute("SELECT * FROM trip_history WHERE user_id = ?", (user_id,))
for trip in trips:
    hazards = db.execute("SELECT * FROM hazards WHERE trip_id = ?", (trip['id'],))
    trip['hazards'] = hazards

# After: Single query with JOIN
trips = db.execute("""
    SELECT t.*, h.* FROM trip_history t
    LEFT JOIN hazards h ON t.id = h.trip_id
    WHERE t.user_id = ?
""", (user_id,))
```

**Benefits**:
- Fewer database queries
- Faster response times
- Better performance

### 4. Connection Pooling (ALREADY IMPLEMENTED âœ…)
**Status**: Already implemented in database_service.py
**Benefit**: Reduced connection overhead
**Impact**: 10-20% performance improvement

```python
# Already in database_service.py
class DatabasePool:
    def __init__(self, db_file, pool_size=5):
        self.pool_size = pool_size
        self.connections = []
        self.lock = threading.Lock()
```

### 5. Batch Operations (MEDIUM)
**Issue**: Multiple individual INSERT/UPDATE
**Solution**: Use batch operations
**Expected Impact**: 40-60% faster bulk operations
**Effort**: 1 hour

```python
# Before: Individual inserts
for hazard in hazards:
    db.execute("INSERT INTO hazards VALUES (?, ?, ?, ?)", 
               (hazard['lat'], hazard['lon'], hazard['type'], hazard['severity']))

# After: Batch insert
db.executemany("INSERT INTO hazards VALUES (?, ?, ?, ?)", 
               [(h['lat'], h['lon'], h['type'], h['severity']) for h in hazards])
```

**Benefits**:
- 40-60% faster bulk operations
- Reduced transaction overhead
- Better performance

### 6. Query Caching (ADVANCED)
**Issue**: Repeated queries
**Solution**: Cache query results
**Expected Impact**: 70-90% cache hit rate
**Effort**: 2 hours

```python
from functools import lru_cache
import time

class QueryCache:
    def __init__(self, ttl=300):
        self.cache = {}
        self.ttl = ttl
    
    def get(self, query, params):
        key = (query, params)
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return value
        return None
    
    def set(self, query, params, value):
        self.cache[(query, params)] = (value, time.time())
```

**Benefits**:
- 70-90% cache hit rate
- Reduced database load
- Faster responses

---

## ðŸ“Š DATABASE METRICS

### Before Optimization
- Average Query Time: 10-50ms
- Slow Queries: 5-10%
- Indexes: 7 (basic)
- Connection Pool: 5

### After Optimization
- Average Query Time: 5-20ms (50-60% reduction)
- Slow Queries: <1%
- Indexes: 15+ (comprehensive)
- Connection Pool: 10

---

## ðŸš€ QUICK WINS (1 HOUR)

### 1. Enable Query Logging
```python
logging.basicConfig(level=logging.DEBUG)
```

### 2. Add Missing Indexes
```sql
CREATE INDEX idx_trip_user_date ON trip_history(user_id, date);
CREATE INDEX idx_hazard_location ON hazards(latitude, longitude);
```

### 3. Optimize N+1 Queries
- Use JOINs instead of multiple queries
- Batch operations together

---

## âœ… VERIFICATION CHECKLIST

- [ ] Query logging enabled
- [ ] Missing indexes added
- [ ] N+1 queries fixed
- [ ] Batch operations implemented
- [ ] Query caching added
- [ ] Connection pooling verified
- [ ] Query performance measured
- [ ] All tests passing

---

## ðŸ“ˆ EXPECTED RESULTS

**Before Optimization**:
- Query Time: 10-50ms
- Slow Queries: 5-10%
- Database Load: High

**After Optimization**:
- Query Time: 5-20ms (50-60% reduction)
- Slow Queries: <1%
- Database Load: Low

---

## ðŸ”— RELATED FILES

- `database_service.py` - Database service
- `voyagr_web.py` - Flask backend
- `production_monitoring.py` - Performance monitoring

---

**Next Steps**: Frontend bundle optimization (Phase 5.7)

